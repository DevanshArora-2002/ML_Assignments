{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP9CjJ5WZ/7Ex2Gy7ajk3NJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DevanshArora-2002/ML_Assignments/blob/main/PartB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jIDKEpsCb_Em",
        "outputId": "5616f91d-316d-4f36-9fbe-32f45900be39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "BlUO8lF1cM4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gzip\n",
        "def images_file_read(file_name):\n",
        "    with gzip.open(file_name, 'r') as f:\n",
        "        magic_number = int.from_bytes(f.read(4), 'big')\n",
        "        image_count = int.from_bytes(f.read(4), 'big')\n",
        "        row_count = int.from_bytes(f.read(4), 'big')\n",
        "        column_count = int.from_bytes(f.read(4), 'big')\n",
        "        image_data = f.read()\n",
        "        images = np.frombuffer(image_data, dtype=np.uint8)\\\n",
        "            .reshape((image_count, row_count, column_count))\n",
        "        return images\n",
        "def labels_file_read(file_name):\n",
        "    with gzip.open(file_name, 'r') as f:\n",
        "        magic_number = int.from_bytes(f.read(4), 'big')\n",
        "        label_count = int.from_bytes(f.read(4), 'big')\n",
        "        label_data = f.read()\n",
        "        labels = np.frombuffer(label_data, dtype=np.uint8)\n",
        "        return labels\n",
        "train_x = images_file_read(\"drive/MyDrive/Assignment3_ML/mnist/train-images-idx3-ubyte.gz\")\n",
        "print(train_x.shape)\n",
        "train_y = labels_file_read(\"drive/MyDrive/Assignment3_ML/mnist/train-labels-idx1-ubyte.gz\")\n",
        "test_x = images_file_read(\"drive/MyDrive/Assignment3_ML/mnist/t10k-images-idx3-ubyte.gz\")\n",
        "print(test_x.shape)\n",
        "test_y = labels_file_read(\"drive/MyDrive/Assignment3_ML/mnist/t10k-labels-idx1-ubyte.gz\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTvkaaOMcZx0",
        "outputId": "5c96321e-654c-41a2-da66-9c67f1d81f0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 28, 28)\n",
            "(10000, 28, 28)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def flatten(train_x):\n",
        "  lst = []\n",
        "  for i in train_x:\n",
        "    samp = i.reshape(i.shape[0] * i.shape[1])\n",
        "    lst.append(samp)\n",
        "  data = np.array(lst)\n",
        "  return data\n",
        "train_x = flatten(train_x)\n",
        "test_x = flatten(test_x)\n",
        "train_x=train_x.T\n",
        "test_x=test_x.T\n",
        "train_y=np.reshape(train_y,(1,train_y.shape[0]))\n",
        "test_y=np.reshape(test_y,(1,test_y.shape[0]))\n",
        "def format_y(data_y):\n",
        "  tr_y=np.zeros(shape=(10,data_y.shape[1]))\n",
        "  for i in range(data_y.shape[1]):\n",
        "    label=data_y[0,i]\n",
        "    tr_y[label,i]=1\n",
        "  return tr_y\n",
        "train_y_trans=format_y(train_y)"
      ],
      "metadata": {
        "id": "L7oVV89xca3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(input):\n",
        "  arr=np.exp(-1*input)\n",
        "  ones=np.ones(shape=input.shape)\n",
        "  arr=ones+arr\n",
        "  arr=np.divide(ones,arr)\n",
        "  return arr\n",
        "def tanh(input):\n",
        "  return np.tanh(input)\n",
        "def relu(input):\n",
        "  arr=np.where(input<0,0,input)\n",
        "  return arr\n",
        "def l_relu(input):\n",
        "  arr=np.where(input<0,input*0.01,input)\n",
        "  return arr\n",
        "def softmax(input):\n",
        "  base_sum=np.max(input,axis=0,keepdims=False)\n",
        "  base_sum=np.reshape(base_sum,newshape=(1,input.shape[1]))\n",
        "  inp=input-base_sum\n",
        "  arr=np.exp(inp)\n",
        "  sumval=np.sum(arr,axis=0)\n",
        "  arr=np.divide(arr,sumval)\n",
        "  return arr\n",
        "def sigmoid_derv(input):\n",
        "  inp=sigmoid(input)\n",
        "  ones=np.ones(shape=input.shape)\n",
        "  arr=np.multiply(inp,ones-inp)\n",
        "  return arr\n",
        "def tanh_derv(input):\n",
        "  arr=np.exp(input)\n",
        "  arr1=np.exp(-input)\n",
        "  arr=arr+arr1\n",
        "  arr=np.multiply(arr,arr)\n",
        "  four=np.ones(shape=input.shape)\n",
        "  four=4*four\n",
        "  arr=np.divide(four,arr)\n",
        "  return arr\n",
        "def relu_derv(input):\n",
        "  arr=input.copy()\n",
        "  arr=np.where(arr<0,0,1)\n",
        "  return arr\n",
        "def l_relu_derv(input):\n",
        "  arr=np.where(input<0,0.01,1)\n",
        "  return arr\n",
        "def softmax_derv(input):\n",
        "  arr=softmax(input)\n",
        "  arr=arr-np.multiply(arr,arr)\n",
        "  return arr\n",
        "def linear(input):\n",
        "  return input\n",
        "def linear_derv(input):\n",
        "  return np.ones(input.shape)\n",
        "def loss(proba,train_y):\n",
        "  val=np.zeros(shape=proba.shape)\n",
        "  for i in range(train_y.shape[1]):\n",
        "    val[i,train_y[0,i]]=1\n",
        "  val=val.T\n",
        "  prob2=proba.T\n",
        "  log1=np.log(prob2)\n",
        "  pr=-1*np.multiply(val,log1)\n",
        "  for i in range(pr.shape[0]):\n",
        "    for j in range(pr.shape[1]):\n",
        "      if(math.isnan(pr[i,j]) or math.isinf(pr[i,j])):\n",
        "        pr[i,j]=0\n",
        "  return np.sum(pr)/(val.shape[1])\n",
        "def cross_loss_back_prop(val,target):\n",
        "  fin=np.multiply(target,softmax(val)-1)\n",
        "  sum=np.sum(fin,axis=0,keepdims=False)\n",
        "  sum=np.reshape(sum,newshape=(1,target.shape[1]))\n",
        "  return sum\n",
        "def initialize(input,output,init,activ):\n",
        "  data={}\n",
        "  data['X']=np.zeros(shape=(input,output))\n",
        "  data['Z']=np.zeros(shape=(output,1))\n",
        "  data['A']=np.zeros(shape=(output,1))\n",
        "  if(init=='zero_init'):\n",
        "    data['weight']=np.zeros(shape=(output,input))\n",
        "    data['bias']=np.zeros(shape=(output,1))\n",
        "  elif(init=='random_init'):\n",
        "    data['weight']=np.random.rand(output,input)\n",
        "    data['weight']*=2\n",
        "    data['weight']=data['weight']-1\n",
        "    data['bias'] = np.zeros(shape=(output, 1))\n",
        "  elif(init=='normal_init'):\n",
        "    data['weight']=np.random.normal(0,1,size=(output,input))\n",
        "    data['bias'] = np.zeros(shape=(output, 1))\n",
        "  if(activ=='sigmoid'):\n",
        "    data['activ']=sigmoid\n",
        "    data['backprop']=sigmoid_derv\n",
        "  elif(activ=='relu'):\n",
        "    data['activ']=relu\n",
        "    data['backprop']=relu_derv\n",
        "  elif(activ=='l_relu'):\n",
        "    data['activ']=l_relu\n",
        "    data['backprop']=l_relu_derv\n",
        "  elif(activ=='tanh'):\n",
        "    data['activ']=tanh\n",
        "    data['backprop']=tanh_derv\n",
        "  elif(activ=='linear'):\n",
        "    data['activ']=linear\n",
        "    data['backprop']=linear_derv\n",
        "  elif(activ=='softmax'):\n",
        "    data['activ']=softmax\n",
        "    data['backprop']=softmax_derv\n",
        "  return data\n",
        "class neural_network:\n",
        "  def __init__(self,N,A,lr=0.001,activ=linear,init='zero_init',epoch=50,batch=100):\n",
        "    self.lr=lr\n",
        "    self.layers_data=[]\n",
        "    self.layers_data.append(initialize(784,A[0],init,activ))\n",
        "    for i in range(1,N):\n",
        "      self.layers_data.append(initialize(A[i-1],A[i],init,activ))\n",
        "    self.final_layer=initialize(A[-1],10,init,'softmax')\n",
        "    self.epoch=epoch\n",
        "    self.batch_size=batch\n",
        "  def forward(self,input,lay):\n",
        "    lay['X']=input\n",
        "    lay['Z']=np.dot(lay['weight'],lay['X'])+lay['bias']\n",
        "    lay['A']=lay['activ'](lay['Z'])\n",
        "    return lay['A']\n",
        "  def backward(self,loss,lay):\n",
        "    multip=np.multiply(loss,lay['backprop'](lay['Z']))\n",
        "    dw=np.dot(multip,lay['X'].T)\n",
        "    db=np.sum(multip,axis=1,keepdims=True)\n",
        "    ret=np.dot(lay['weight'].T,multip)\n",
        "    return dw,db,ret\n",
        "  def fit(self,input,target):\n",
        "    no_samples=input.shape[1]\n",
        "    for i in range(self.epoch):\n",
        "      print(i)\n",
        "      st=0\n",
        "      while(st+self.batch_size<no_samples):\n",
        "        inp=input[:,st:min(st+self.batch_size,no_samples)]\n",
        "        for j in range(len(self.layers_data)):\n",
        "          lay=self.layers_data[j]\n",
        "          out=self.forward(inp,lay)\n",
        "          inp=out\n",
        "        out=self.forward(inp,self.final_layer)\n",
        "        tar=target[:,st:min(st+self.batch_size,no_samples)]\n",
        "        diff=out-tar\n",
        "        dw=np.dot(diff,self.final_layer['X'].T)\n",
        "        db=np.sum(diff,axis=1,keepdims=True)\n",
        "        self.final_layer['weight']=self.final_layer['weight']-self.lr*(1/(no_samples-self.batch_size))*dw\n",
        "        self.final_layer['bias']=self.final_layer['bias']-self.lr*(1/(no_samples-self.batch_size))*db\n",
        "        loss=np.dot(self.final_layer['weight'].T,diff)\n",
        "        for j in range(len(self.layers_data)-1,-1,-1):\n",
        "          lay = self.layers_data[j]\n",
        "          dw, db, loss = self.backward(loss, lay)\n",
        "          lay['weight'] = lay['weight'] - self.lr * (1 / self.batch_size) * dw\n",
        "          lay['bias'] = lay['bias'] - self.lr * (1 / self.batch_size) * db\n",
        "        st+=self.batch_size\n",
        "  def predict_proba(self,train_x):\n",
        "    inp=train_x\n",
        "    for lay in self.layers_data:\n",
        "      out=self.forward(inp,lay)\n",
        "      inp=out\n",
        "    return self.forward(inp,self.final_layer)\n",
        "  def predict(self,data_x):\n",
        "    pred=[]\n",
        "    for i in range(data_x.shape[1]):\n",
        "      inp=data_x[:,i]\n",
        "      inp=np.reshape(inp,(len(inp),1))\n",
        "      for j in range(len(self.layers_data)):\n",
        "        lay = self.layers_data[j]\n",
        "        out = self.forward(inp, lay)\n",
        "        inp = out\n",
        "      out = self.forward(inp, self.final_layer)\n",
        "      max_ind=0\n",
        "      for j in range(10):\n",
        "        if out[max_ind,0]<out[j,0]:\n",
        "          max_ind=j\n",
        "      pred.append(max_ind)\n",
        "    return np.array([pred])\n",
        "  def score(self,train_x,train_y):\n",
        "    pred=self.predict(train_x)\n",
        "    tot=train_y.shape[1]\n",
        "    corr=0\n",
        "    for i in range(tot):\n",
        "      if pred[0,i]==train_y[0,i]:\n",
        "        corr+=1\n",
        "    return corr/tot\n",
        "  def loss(self,proba, train_y):\n",
        "    val = np.zeros(shape=proba.shape)\n",
        "    for i in range(len(train_y)):\n",
        "      val[i, train_y[i]] = 1\n",
        "    val = val.T\n",
        "    val2=1-val\n",
        "    prob2 = proba.T\n",
        "    log1 = np.log(prob2)\n",
        "    log2 = np.log(1-prob2)\n",
        "    pr = -1 * np.multiply(val, log1)\n",
        "    pr2=-1*np.multiply(val2,log2)\n",
        "    sum=np.sum(pr)+np.sum(pr2)\n",
        "    return sum/(val.shape[1])\n",
        "  def partial_fit(self,input,target):\n",
        "    no_samples = input.shape[1]\n",
        "    st = 0\n",
        "    while (st + self.batch_size < no_samples):\n",
        "      inp = input[:, st:min(st + self.batch_size, no_samples)]\n",
        "      for j in range(len(self.layers_data)):\n",
        "        lay = self.layers_data[j]\n",
        "        out = self.forward(inp, lay)\n",
        "        inp = out\n",
        "      out = self.forward(inp, self.final_layer)\n",
        "      tar = target[:, st:min(st + self.batch_size, no_samples)]\n",
        "      diff = out - tar\n",
        "      dw = np.dot(diff, self.final_layer['X'].T)\n",
        "      db = np.sum(diff, axis=1, keepdims=True)\n",
        "      self.final_layer['weight'] = self.final_layer['weight'] - self.lr * (1 / (no_samples - self.batch_size)) * dw\n",
        "      self.final_layer['bias'] = self.final_layer['bias'] - self.lr * (1 / (no_samples - self.batch_size)) * db\n",
        "      loss = np.dot(self.final_layer['weight'].T, diff)\n",
        "      for j in range(len(self.layers_data) - 1, -1, -1):\n",
        "        lay = self.layers_data[j]\n",
        "        dw, db, loss = self.backward(loss, lay)\n",
        "        lay['weight'] = lay['weight'] - self.lr * (1 / self.batch_size) * dw\n",
        "        lay['bias'] = lay['bias'] - self.lr * (1 / self.batch_size) * db\n",
        "      st += self.batch_size"
      ],
      "metadata": {
        "id": "sGIGpoxScfxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x=train_x/255\n",
        "test_x=test_x/255"
      ],
      "metadata": {
        "id": "gUz78D6Gcv_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params=[\n",
        "    'sigmoid',\n",
        "    'relu',\n",
        "    'l_relu',\n",
        "    'tanh',\n",
        "    'linear'\n",
        "]\n",
        "models_info={}\n",
        "for i in params:\n",
        "    l1=[]\n",
        "    l2=[]\n",
        "    n_net=neural_network(N=4,A=[256,128,64,32],lr=0.01,activ=i,init='random_init',epoch=100,batch=128)\n",
        "    for j in range(100):\n",
        "        n_net.partial_fit(train_x,train_y_trans)\n",
        "        prob1=n_net.predict_proba(train_x)\n",
        "        prob2=n_net.predict_proba(test_x)\n",
        "        l1.append(loss(prob1.T,train_y))\n",
        "        l2.append(loss(prob2.T,test_y))\n",
        "    x=[i for i in range(100)]\n",
        "    #plt.xlabel(\"Epochs\")\n",
        "    #plt.ylabel(\"Loss\")\n",
        "    #plt.plot(x,l1)\n",
        "    #plt.plot(x,l2)\n",
        "    #string=\"Plots_B/\"+i+\" loss\"\n",
        "    #plt.savefig(string)\n",
        "    #plt.cla()\n",
        "    models_info[i]=n_net\n",
        "    print(i,end=\" \")\n",
        "    print(\"Training Loss {}\".format(n_net.score(train_x,train_y)))\n",
        "    print(\"Testing Loss {}\".format(n_net.score(test_x,test_y)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bp_tpZTYclll",
        "outputId": "9258f9b6-5038-4a92-b965-567484839102"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sigmoid Training Loss 0.9090333333333334\n",
            "Testing Loss 0.9043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:58: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: RuntimeWarning: invalid value encountered in multiply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "relu Training Loss 0.20395\n",
            "Testing Loss 0.2047\n",
            "l_relu Training Loss 0.9353833333333333\n",
            "Testing Loss 0.9238\n",
            "tanh Training Loss 0.9342666666666667\n",
            "Testing Loss 0.7741\n",
            "linear Training Loss 0.09871666666666666\n",
            "Testing Loss 0.098\n"
          ]
        }
      ]
    }
  ]
}